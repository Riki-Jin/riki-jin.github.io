---
layout:     post
title:      The comparison of different DE Analysis methods in microbiome sequencing data
subtitle:   Comparison of Type-I error rate and power
date:       2019-08-22
author:     Riki
header-img: img/microbiome.png
catalog: true
tags:
    - study
---

## INTRODUCTION (Karen)
>_"A microbiota is an "ecological community of commensal, symbiotic and pathogenic microorganisms" found in and on all multicellular organisms studied to date from plants to animals. A microbiota includes bacteria, archaea, protists, fungi and viruses. Microbiota have been found to be crucial for immunologic, hormonal and metabolic homeostasis of their host. The synonymous term microbiome describes either the collective genomes of the microorganisms that reside in an environmental niche or the microorganisms themselves."_ (Wikipedia)

The microbiome represents all microbial species that are present in a particular environment. Advances in sequencing technology have facilitated the characterization of microbial communities in various environments and the identification of species that would be difficult, if not impossible, to cultivate through traditional methods. 

Alterations in microbiome composition have been linked to numerous health conditions, such as inflammatory bowel disease, obesity, and respiratory illnesses (Young 2017). In the context of human health, microbiome studies seek to understand the interaction between host biological and environmental factors and microbiome structure with the future goal of developing methods to treat disease through the modulation of microbial composition (Xia and Sun 2017). One method of assessing microbe-disease associations is through differential abundance testing, which identifies differences in mean abundances of various species between healthy and diseased individuals.

Common sequencing methods used to characterize microbiomes are marker gene surveys and whole genome shotgun sequencing. Marker gene surveys sequence a gene with high sequence variability between species, often the 16S rRNA gene, to identify microbes present within the sample; in contrast, whole genome sequencing does not have such restrictions. Reads with a specified amount of sequence similarity, typically ~97%, are clustered into operation taxonomic units (OTUs), which are representative of species (Jovel et al. 2016). The resulting data are organized into matrices with counts representing the abundance of each OTU in each sample.
	
Microbiome sequencing data is typically sparse with a high percentage of zero counts. Various sources have estimated this value to be approximately between 50-90% of the total sample counts (Weiss et al. 2017). These zeros may indicate true absence of the species in the sample or they may be caused by insufficient sequencing depth, an example of which is a rare species that is approaching the limit of detection by the machine. Additionally, the number of detected species is rarely saturated and further sequencing typically leads to more observed species (Weiss et al. 2017).

In this context, library size is defined as the total counts per sample. Differential sequencing efficiency can result in large differences in library sizes between samples, which can obscure true differences in species abundance. For example, if Sample A has five times the library size as that of Sample B, all of the OTUs in Sample A would appear to be differentially abundant even if the relative abundances between the two groups are similar, due to the fact that the absolute counts in Sample A would much higher. This library size effect can be reduced by using normalization methods, which remove biases and variation introduced by sampling and sequencing processes.

The goal of this study is to compare the performance of common normalization and differential abundance testing methods used for the analysis of microbiome data when sample library sizes are similar versus when they are different. The efficacy of these methods will be evaluated in terms of their type I error rates and power using data simulated from a zero-inflated negative binomial distribution. Previous studies have used existing microbiome datasets to compare methods in the presence of batch effects, but have not thoroughly addressed the library size issue (McMurdie and Holmes 2014; Pereira et al. 2018; Weiss et al. 2017).




## DATA AND SCENARIOS

#### ZINB distribution

ZINB(Zero-Inflated Negative Binomial) distribution is often used to simulate OTU count data according to its zero-inflated and overdispersion features:

There are three parameters in a ZINB distribution:
- μ : mean of the non-ZI part
- θ : measure of dispersion
- π : the probability of the ZI part

We use a Gamma-Poisson mixture model to generate negative binomial variables:
```
rZINB <- function(n, param=NULL, m, t, p) 
  if (!is.null(param)) {m = param[1]; t = param[2]; p = param[3]}
  
  rvec <- rgamma(n, shape = m/t, rate = 1/t)
  rvec <- rpois(n, rvec)
  pvec <- rbinom(n, 1, 1-p)
  xvec <- rvec * pvec
  return(xvec)
}
```

After this Gamma-Poisson mixture process, we could get a random variable which has following distribution:  

![mRJszD.png](https://s2.ax1x.com/2019/08/26/mRJszD.png)

The parameter μ and θ can proportional reflects mean and variance of the random variable. For the non-ZI part (the simple NB part), we can calculate that:
- mean = μ
- variance = μ(θ+1)


#### Scenarios

First of all, we want to confirm our settings of the tested data:
- number of OTUs/taxa (rows of the matrix): 100
- number of samples (columns of the matrix): 2 groups * 50 samples/group = 100
- Each item of the matrix is generated from a ZINB distribution independently with the parameters: **μ = 10, θ = 5, π = 0.75**. These parameters were derived from an oral microbiome sequencing dataset from the ZOE2.0 study conducted at the University of North Carolina.
- Fold changes difference between "small" and "large" library sizes: 4

We designed three different scenarios of our generated data:
- **Scenario A**: Strictly designed same library size
- **Scenario B**: Completely random library size
- **Scenario C**: Mixed samples containing two obvious different library sizes
- **Scenario D**: Mixed samples containing extremely small and large library sizes

To autualize these scenarios, we first generate a very large matrix with 1000000 (one million) samples, and then plot the histogram of library sizes of these one million samples:

![mhlKbR.png](https://s2.ax1x.com/2019/08/26/mhlKbR.png)  

About this histogram, we could find that the median of library sizes is about 250, while there are enough samples which have library size of approximately 100 or 400. Therefore, we would choose library size = 250 for scenario A, and library size = 100/400 for scenario C. However, for scenario B, it is unwanted to generate a "very large matrix" first.

For example, this is how we generate data of scenario C:
(of course, we need a mixture step after doing this, but in order to save time, I would like to enter a slightly larger sample size (say 200) first, and then do the resampling for a few times)
```
genC <- function(simsamples){
  simnums <- rZINB(numtaxa*1000000, m=10, t=5, p=0.75)    # numtaxa=100
  samples <- matrix(simnums, byrow=T, nrow=numtaxa)    
  samprank <- samples[, order(colSums(samples))]
  libsizes <- colSums(samprank)
  low <- samprank[, which(libsizes==100)[1]:(which(libsizes==100)[1]+simsamples-1)]
  high <- samprank[, which(libsizes==400)[1]:(which(libsizes==400)[1]+simsamples-1)]
  return(cbind(low, high))
}
```

About scenario D, if we look back to the histogram, we may be curious about the most extreme library sizes. After extracting the smallest and the largest 100 library sizes among all 1000000 samples, we could get:
> 35 54 55 55 55 55 55 56 56 57 57 58 59 60 60 62 62 62 63 63 63 64 64 64 65 65 65 65 65 66 66 66 66 66 66 66 66 67 67 67 67 67 67 67 67 67 68 68 68 68 69 69 69 69 69 69 69 69 70 70 70 70 70 71 71 71 71 71 71 71 71 71 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 73 73 73 73 73 73 73 73 73
> 504 505 505 505 505 505 506 506 507 507 507 507 507 507 508 508 508 508 508 509 509 509 509 510 510 510 511 511 511 511 512 512 512 512 513 513 513 514 514 514 514 514 514 514 515 516 517 517 517 517 517 517 518 518 518 518 519 519 519 519 520 520 520 520 520 521 522 523 523 523 524 525 527 528 528 528 529 529 529 529 532 533 534 536 536 538 539 540 540 540 542 547 551 553 554 556 557 561 564 573 592  

We then resample 50 samples from each two extreme groups for several times, to make the scenario D dataset. This dataset is so extreme that its realistic significance remains unclear. We make this artificial dataset only because of the motivation of comparing different methods under different library size composition.

## MATERIALS AND METHODS

We often want to cluster a batch of samples or find differential expressed OTUs between different groups. Due to the huge difference of library sizes among the samples, large amount of zeros and over-dispersion structure of microbiome sequencing data, it must be normalized. Otherwise, huge mistake may be caused.

- *Cluster*: Using certain normalization method + distance metrics (e.g. Bray-Curtis, Unifrac...)
- *Differential Analysis*:  Using certain differential abundance test, which contains a normalization preprocessing method.

In this passage, we would only talk about differential analysis.

#### Normalization Methods

#### Differential abundance test

#### About "Pseudocount"  

> A pseudocount is a count added to observed data in order to change the probability in a model of those data, which is known not to be zero, to being negligible rather than being zero.  
In any observed data set or sample there is the possibility, especially with low-probability events and/or small data sets, of a possible event not occurring. Its observed frequency is therefore 0, implying a probability of 0. This is an oversimplification and is often unhelpful, particularly in probability-based machine learning techniques such as artificial neural networks and hidden Markov models.By artificially adjusting the probability of rare (but not impossible) events so those probabilities are not exactly zero, we avoid the zero-frequency problem.   
The simplest approach is to add "1" to each observed number of events including the zero-count one. This is sometimes called "Laplace's rule" (more formally known as Laplace's rule of succession).A more complex approach is to estimate the probability of the events from other factors and adjust accordingly.

Among our methods, **DESeq2**, **UQ + edgeR** and **RLE + edgeR** can't accept zero count. The simplest approach is to add "1" to each observed number of events including the zero-count one. However, this may lead to a huge inflation of type-I error. <sup>[2]</sup>    

Therefore, we try to change this "1" into other constants, such as 10 or 0.01. In fact, add 0.01 is a recorded method for edgeR in [Sophie Weiss's paper](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-017-0237-y)

#### Steps of "Type-I error rate test" and "Power test"

1. Generate independent data matrices, randomly select one OTU for each matrix.
2. Add signal to this OTU.
3. Use various differential abundance analysis methods above to test the OTU in step 1&2. Then extract the raw-p value.
4. Go back to step 2 and vary the intensity of the signal. (fold change = 1,2,5,10,20).  
*(Note: When the fold change = 1, it means that there are no signal added into the matrix, so we can test the type-I error of different methods under this situation.)*  
5. For each method, calculate the proportion that the raw-p value lower than 0.05 out of all the matrices.  
*(Note: The cutoff 0.05 can be substituted by 0.04, 0.03, 0.02, 0.01 in our plot, in order to check the uniformity of raw-p value of different methods.)*




## RESULTS AND DISCUSSION
![Powertest_result_summary.png](https://i.loli.net/2019/08/21/49PQFLtcWmwDUuK.png)

This plot shows different traits of different test methods:
- **Wilcoxon**: Nearly no power at all due to the large amount of zeros(ties).
- **T-test**: Apparently lower power than RNA-seq analysis tools (DESeq2, edgeR) and MGS (metagenomeSeq).
- **DESeq2**: High power but also high type-I error rate. Turning pseudocount constant "1" into "10" would efficiently suppress the type-I error rate, while the cost is lower power.
- **edgeR**:  
- **CSS + MGS**: Have controllable type-one error when there is no signal, meanwhile have high power when there is large fold change signal.

Note:
1. In scenario A with fold change 1 (no signal), rarefy and proportion actually did nothing. Therefore, "T-test"/"Rarefy+T-test"/"Prop+T-test" have exactly same P-value and type-one error rate. (Please consult the data below)

2.


## CONCLUSION

We confirm that metagenome (MGS fitfeaturemodel) is a good method to analyze our simulation data, which can hold low type-I error when no signal is added, and reach high power when obvious signal (10x) is added. In fact, in the real data, 10x is never an exaggerate fold change, we can even see more than 1000x fold change in microbiome data. [Please check this](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxzaWRkaGFydGhhbWFuZGFsMTk4NXxneDo1OGEyMDlkZDBiOWQ2ZmI1). In addition, metagenome method behaves very stable among different scenarios, which means it is not easily influenced by mixed library sizes. Therefore, we could expect metagenome to be appropriate and mighty when dealing with the microbiome sequencing data.

DESeq2 is a tool designed for RNA-seq data, we would perform RLE method as a normalization process. It has high power when there are large differences between two sample groups, but it also have abnormal inflation type-I error rate when no signal is added.  Using larger value as the pseudocount constant will alleviate this sharp problem, but it is after all a tradeoff strategy, because the power would simultaneously drop as well.

edgeR is another common tool for RNA-seq data, 

Two sample T-test is one of the most famous and classic differential test method in history. 

Wilcoxon rank test is the worst test among all. As a nonparametric method, it is especially inappropriate for microbiome data because of the existence of huge amount of zeros. All the zeros would be regarded as "ties" in the Wilcoxon rank test, and since these zeros are widely interspersed among all samples in 2 groups, Wilcoxon rank test could hardly distinguish the difference of two groups. When we try to add signals to one of the groups, Wilcoxon rank test still behaves terribly bad even the signal is very large, because zero times any number is still zero.  
Use a zero imputation method may possibly solve this problem, but it changes the raw data and doesn't have a one-to-one correspondence relationship (bijection). Therefore, we didn't take Wilcoxon test into account when plotting.
