---
layout:     post
title:      The comparison of different DE Analysis methods in microbiome sequencing data
subtitle:   Comparison of Type-I error rate and power
date:       2019-08-22
author:     Riki
header-img: img/microbiome.png
catalog: true
tags:
    - study
---

## INTRODUCTION (By Karen)
>_"A microbiota is an "ecological community of commensal, symbiotic and pathogenic microorganisms" found in and on all multicellular organisms studied to date from plants to animals. A microbiota includes bacteria, archaea, protists, fungi and viruses. Microbiota have been found to be crucial for immunologic, hormonal and metabolic homeostasis of their host. The synonymous term microbiome describes either the collective genomes of the microorganisms that reside in an environmental niche or the microorganisms themselves."_ (Wikipedia)

The microbiome represents all microbial species that are present in a particular environment. Advances in sequencing technology have facilitated the characterization of microbial communities in various environments and the identification of species that would be difficult, if not impossible, to cultivate through traditional methods. 

Alterations in microbiome composition have been linked to numerous health conditions, such as inflammatory bowel disease, obesity, and respiratory illnesses (Young 2017). In the context of human health, microbiome studies seek to understand the interaction between host biological and environmental factors and microbiome structure with the future goal of developing methods to treat disease through the modulation of microbial composition (Xia and Sun 2017). One method of assessing microbe-disease associations is through differential abundance testing, which identifies differences in mean abundances of various species between healthy and diseased individuals.

Common sequencing methods used to characterize microbiomes are marker gene surveys and whole genome shotgun sequencing. Marker gene surveys sequence a gene with high sequence variability between species, often the 16S rRNA gene, to identify microbes present within the sample; in contrast, whole genome sequencing does not have such restrictions. Reads with a specified amount of sequence similarity, typically ~97%, are clustered into operation taxonomic units (OTUs), which are representative of species (Jovel et al. 2016). The resulting data are organized into matrices with counts representing the abundance of each OTU in each sample.
	
Microbiome sequencing data is typically sparse with a high percentage of zero counts. Various sources have estimated this value to be approximately between 50-90% of the total sample counts (Weiss et al. 2017). These zeros may indicate true absence of the species in the sample or they may be caused by insufficient sequencing depth, an example of which is a rare species that is approaching the limit of detection by the machine. Additionally, the number of detected species is rarely saturated and further sequencing typically leads to more observed species (Weiss et al. 2017).

In this context, library size is defined as the total counts per sample. Differential sequencing efficiency can result in large differences in library sizes between samples, which can obscure true differences in species abundance. For example, if Sample A has five times the library size as that of Sample B, all of the OTUs in Sample A would appear to be differentially abundant even if the relative abundances between the two groups are similar, due to the fact that the absolute counts in Sample A would much higher. This library size effect can be reduced by using normalization methods, which remove biases and variation introduced by sampling and sequencing processes.

The goal of this study is to compare the performance of common normalization and differential abundance testing methods used for the analysis of microbiome data when sample library sizes are similar versus when they are different. The efficacy of these methods will be evaluated in terms of their type I error rates and power using data simulated from a zero-inflated negative binomial distribution. Previous studies have used existing microbiome datasets to compare methods in the presence of batch effects, but have not thoroughly addressed the library size issue (McMurdie and Holmes 2014; Pereira et al. 2018; Weiss et al. 2017).




## DATA AND SCENARIOS

#### ZINB distribution

ZINB(Zero-Inflated Negative Binomial) distribution is often used to simulate OTU count data according to its zero-inflated and overdispersion features:

There are three parameters in a ZINB distribution:
- μ : mean of the non-ZI part
- θ : measure of dispersion
- π : the probability of the ZI part

We use a Gamma-Poisson mixture model to generate negative binomial variables:
```
rZINB <- function(n, param=NULL, m, t, p) 
  if (!is.null(param)) {m = param[1]; t = param[2]; p = param[3]}
  
  rvec <- rgamma(n, shape = m/t, rate = 1/t)
  rvec <- rpois(n, rvec)
  pvec <- rbinom(n, 1, 1-p)
  xvec <- rvec * pvec
  return(xvec)
}
```

After this Gamma-Poisson mixture process, we could get a random variable which has following distribution:  

![mRJszD.png](https://s2.ax1x.com/2019/08/26/mRJszD.png)

The parameter μ and θ can proportional reflects mean and variance of the random variable. For the non-ZI part (the simple NB part), we can calculate that:
- mean = μ
- variance = μ(θ+1)


#### Scenarios

First of all, we want to confirm our settings of the tested data:
- number of OTUs/taxa (rows of the matrix): 100
- number of samples (columns of the matrix): 2 groups * 50 samples/group = 100
- Each item of the matrix is generated from a ZINB distribution independently with the parameters: **μ = 10, θ = 5, π = 0.75**. These parameters were derived from an oral microbiome sequencing dataset from the ZOE2.0 study conducted at the University of North Carolina.
- Fold changes difference between "small" and "large" library sizes: 4

We designed three different scenarios of our generated data:
- **Scenario A**: Strictly designed same library size
- **Scenario B**: Completely random library size
- **Scenario C**: Mixed samples containing two obvious different library sizes
- **Scenario D (Optional)**: Mixed samples containing extremely small and large library sizes

To autualize these scenarios, we first generate a very large matrix with 1000000 (one million) samples, and then plot the histogram of library sizes of these one million samples:

![mhlKbR.png](https://s2.ax1x.com/2019/08/26/mhlKbR.png)  

About this histogram, we could find that the median of library sizes is about 250, while there are enough samples which have library size of approximately 100 or 400. Therefore, we would choose library size = 250 for scenario A, and library size = 100/400 for scenario C. However, for scenario B, it is unwanted to generate a "very large matrix" first.

For example, this is how we generate data of scenario C:
(of course, we need a mixture step after doing this, but in order to save time, I would like to enter a slightly larger sample size (say 200) first, and then do the resampling for a few times)
```
genC <- function(simsamples){
  simnums <- rZINB(numtaxa*1000000, m=10, t=5, p=0.75)    # numtaxa=100
  samples <- matrix(simnums, byrow=T, nrow=numtaxa)    
  samprank <- samples[, order(colSums(samples))]
  libsizes <- colSums(samprank)
  low <- samprank[, which(libsizes==100)[1]:(which(libsizes==100)[1]+simsamples-1)]
  high <- samprank[, which(libsizes==400)[1]:(which(libsizes==400)[1]+simsamples-1)]
  return(cbind(low, high))
}
```

About scenario D, if we look back to the histogram, we may be curious about the most extreme library sizes. After extracting the smallest and the largest 100 library sizes among all 1000000 samples, we could get:
> 35 54 55 55 55 55 55 56 56 57 57 58 59 60 60 62 62 62 63 63 63 64 64 64 65 65 65 65 65 66 66 66 66 66 66 66 66 67 67 67 67 67 67 67 67 67 68 68 68 68 69 69 69 69 69 69 69 69 70 70 70 70 70 71 71 71 71 71 71 71 71 71 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 72 73 73 73 73 73 73 73 73 73

and

> 504 505 505 505 505 505 506 506 507 507 507 507 507 507 508 508 508 508 508 509 509 509 509 510 510 510 511 511 511 511 512 512 512 512 513 513 513 514 514 514 514 514 514 514 515 516 517 517 517 517 517 517 518 518 518 518 519 519 519 519 520 520 520 520 520 521 522 523 523 523 524 525 527 528 528 528 529 529 529 529 532 533 534 536 536 538 539 540 540 540 542 547 551 553 554 556 557 561 564 573 592  

We then resample 50 samples from each two extreme groups for several times, to make the scenario D dataset. This dataset is so extreme that its realistic significance remains unclear. We make this artificial dataset only because of the motivation of comparing different methods under different library size composition. **However, we would take this scenario into account after comparing the scenario A,B and C.**

## MATERIALS AND METHODS

We often want to cluster a batch of samples or find differential expressed OTUs between different groups. Due to the huge difference of library sizes among the samples, large amount of zeros and over-dispersion structure of microbiome sequencing data, it must be normalized. Otherwise, huge mistake may be caused.

- *Cluster*: Using certain normalization method + distance metrics (e.g. Bray-Curtis, Unifrac...)
- *Differential Analysis*:  Using certain differential abundance test, which contains a normalization preprocessing method.

In this passage, we would only talk about differential analysis.

#### Normalization Methods (Karen)

Seven normalization methods were evaluated in this essay:
- **Rarefying**: Rarefying equalizes library sizes by subsampling the data to an even depth (McMurdie and Holmes 2014). First, a minimum library size is selected; samples with fewer reads than the minimum threshold are discarded. Typically, the minimum library size is chosen to be equal to that of the smallest library considered not defective, which introduces some level of subjectivity. Afterward, the remaining samples are subsampled without replacement to reach the minimum library size. 

- **Proportion scaling**: Proportion scaling, also commonly referred to as total sum scaling, adjusts library sizes by scaling the counts each sample by the sample total. We would get a proportional/compositional data matrix instead of count data matrix, with the total sum of each column (sample) is 1.

- **Upper quartile scaling (UQ)**: In UQ scaling, the 75th percentile of counts that are non-zero in at least one sample is used to scale library sizes (Bullard et al. 2010). An advantage of this method over proportion scaling is that UQ is less influenced by the presence of extremely high counts.

- **Cumulative sum scaling (CSS)**: In CSS, the scaling factor is calculated as a cumulative sum of OTU counts up to a data determined quantile. As with UQ, the CSS approach attempts to reduce the influence of preferentially sampled OTUs, but is more flexible with a quantile threshold determined by individual sample count distributions (Paulson et al. 2013). CSS was implemented using the metagenomeSeq package (version 1.24.1).

- **Trimmed mean of M-values (TMM)**: TMM assumes that the majority of OTUs are not differentially abundant. The scaling factor is calculated as a weighted mean of the log fold change between sample pairs after excluding OTUs with the largest log fold changes and highest absolute counts (Weiss et al. 2017).

- **TMM with zero pairing (TMMwzp)**: TMMwzp is a modified version of TMM that has better performance when the data have a high proportion of zeros. While TMM ignores OTUs with zero count in either sample, TMMwzp reuses positive counts for such OTUs to increase the number of features that can be compared (Robinson and Smyth 2019).

- **Relative log expression (RLE)**: Similar to TMM, RLE assumes that most OTUs are not differentially abundant. A pseudo-reference for each OTU is calculated as the geometric mean of the counts across all samples. The scaling factor is the median of all the ratios between sample counts and the pseudo-reference (Love et al. 2014)

#### Differential abundance test (Karen)

Five differential abundance testing methods were evaluated in combination with the normalization techniques mentioned above. These methods aim to detect differences in relative taxon abundances between sample groups. Approaches considered are summarized in Table 1 and include two-sample tests (t-test, Wilcoxon rank sum test), commonly used RNA-seq analysis tools (DESeq2, edgeR), and a method developed specifically for microbiome sequencing data (metagenomeSeq).

**However, because of the terribly low power of Wilcoxon tests, we would not show it in our final result table and plots.**

| Testing method | Normalization performed | Statistical Model |
| -------------- | ----------------------- | ----------------- |
| Two sample T-test | None, Prop, Rarefy   | Normal distribution T-test|
| Wilcoxon Test  | None, Prop, Rarefy      | Wilcoxon rank test |
| DESeq2  | RLE | Models counts with negative binomial generalized linear models (GLMs). Dispersion estimates are shrunken toward fitted values with empirical Bayes and log fold change of abundances between groups are compared with a Wald test (Love et al. 2014).|
| edgeR | TMM, TMMwzp, UQ, RLE | Models counts with a negative binomial GLMs. Estimated dispersions are shrunken toward a common value with empirical Bayes. Performs an exact test to compare relative mean abundances between groups (Robinson et al. 2009). |
| metagenomeSeq | CSS | Models counts with a zero-inflated log-normal distribution. Parameter estimates are shrunken with empirical Bayes and a moderated t-statistic is used (Paulson et al. 2013). |




#### About "Pseudocount"  

> A pseudocount is a count added to observed data in order to change the probability in a model of those data, which is known not to be zero, to being negligible rather than being zero.  
In any observed data set or sample there is the possibility, especially with low-probability events and/or small data sets, of a possible event not occurring. Its observed frequency is therefore 0, implying a probability of 0. This is an oversimplification and is often unhelpful, particularly in probability-based machine learning techniques such as artificial neural networks and hidden Markov models.By artificially adjusting the probability of rare (but not impossible) events so those probabilities are not exactly zero, we avoid the zero-frequency problem.   
The simplest approach is to add "1" to each observed number of events including the zero-count one. This is sometimes called "Laplace's rule" (more formally known as Laplace's rule of succession).A more complex approach is to estimate the probability of the events from other factors and adjust accordingly. [(From academic wiki)](https://en.academic.ru/dic.nsf/enwiki/981488)

Among our methods, **DESeq2**, **UQ + edgeR** and **RLE + edgeR** can't accept zero count. The simplest approach is to add "1" to each observed number of events including the zero-count one. However, this may lead to a huge inflation of type-I error <sup>[2]</sup>. From this plot, we could easily find that pseudocount is an important reason of the inflation of false positive rate (type-I error).
![nFGdk6.png](https://s2.ax1x.com/2019/09/03/nFGdk6.png)


Therefore, we tried to change this "1" into other constants, such as 10 or 0.01. In fact, add 0.01 is a recorded method for edgeR in [Sophie Weiss's paper](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-017-0237-y)

We first tried larger constants, say "5" or "10":
(*Note: We don't need to apply any pseudocount method on edgeR/TMM+edgeR/TMMwzp+edgeR, we put these three methods here only to compare*)
![nFNSgS.png](https://s2.ax1x.com/2019/09/03/nFNSgS.png)

Then we tried smaller constants, say "0.1" or "0.01":
（*Note: Since DESeq2 can't accept non-integer input, we removed it from the plot*）
![nFy5Ox.png](https://s2.ax1x.com/2019/09/03/nFy5Ox.png)

After these comparison, we found that type-I error rate can be effectively by changing the constant. For DESeq2, "10" has lower type-I error rate but also lower power, while "1" is a more commonly used constant. Therefore, we would use both and compare them. For edgeR, now that "0.01" is a recorded method in [Sophie Weiss's paper](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-017-0237-y) and it indeed has more controllable inflation of type-I error, we would use "0.01" instead of "1".



#### Steps of "Type-I error rate test" and "Power test"

1. Generate independent data matrices, randomly select one OTU for each matrix.
2. Add signal to this OTU.
3. Use various differential abundance analysis methods above to test the OTU in step 1&2. Then extract the raw-p value.
4. Go back to step 2 and vary the intensity of the signal. (fold change = 1,2,5,10,20).  
*(Note: When the fold change = 1, it means that there are no signal added into the matrix, so we can test the type-I error of different methods under this situation.)*  
5. For each method, calculate the proportion that the raw-p value lower than 0.05 out of all the matrices.  
*(Note: The cutoff 0.05 can be substituted by 0.04, 0.03, 0.02, 0.01 in our plot, in order to check the uniformity of raw-p value of different methods.)*




## RESULTS AND DISCUSSION

Here is my result:

[Result](https://pan.baidu.com/s/1afuad1x-Rj0H4uzzX0wWaw)

The result is a csv file with 208 columns:
![nizgdf.png](https://s2.ax1x.com/2019/09/03/nizgdf.png)

- Column 1: Scenarios: A, B, C, D
- Column 2: Methods: T-test, Prop+T-test, Rare+T-test, edgeR, TMM+edgeR, TMMwzp+edgeR, RLE+edgeR, UQ+edgeR, DESeq2(1), DESeq2(10), MGS    
*(Note: DESeq2(1) means DESeq2 method with "1" constant for pseudocount)*
- Column 3: Fold changes: 1, 2, 5, 10, 20      
*(Note: These signals are applied on the same random OTU, fold change = 1 means no statistical difference between two groups)*
- Columns 4-8: Powers using cutoff 0.01/0.02/0.03/0.04/0.05
- Columns 9-208: Raw p-values of 200 samples under this scenario, fold change and method
*(Note: P-values of the same column can be compared. For example, in the "P-1" column, we could find that the p-values of row 1 and row 6 are exactly same.)*


We want to compare different methods, so we plot as:
![nA6RUK.png](https://s2.ax1x.com/2019/09/03/nA6RUK.png)

This plot shows different traits of different test methods:
- **Wilcoxon**: Nearly no power at all due to the large amount of zeros(ties).
- **T-test**: Apparently lower power than RNA-seq analysis tools (DESeq2, edgeR) and MGS (metagenomeSeq).
- **DESeq2**: High power but also high type-I error rate. Turning pseudocount constant "1" into "10" would efficiently suppress the type-I error rate, while the cost is lower power. (However, the power is still higher then any other methods when the signals are not so obvious)
- **edgeR**:  
- **CSS + MGS**: Have controllable type-one error when there is no signal, meanwhile have high power when there is large fold change signal.

Note: In scenario A with fold change 1 (no signal), rarefy and proportion actually did nothing. Therefore, "T-test"/"Rarefy+T-test"/"Prop+T-test" have exactly same P-value and type-one error rate. (Please consult row 1, row 6 and row 11 of the Result.csv data)


In short, we could summary our methods as:
(★★★★★ = Very Good, ★★★★ = Good, ★★★ = Fair, ★★ = Bad, ★ = Very Bad, ☆ = Terrible)

| Testing method | Performance when no signal <br> (Fold Change = 1) | Sensitivity of small signal <br> (Fold Change = 2,5) | Stability of large signal <br> (Fold Change = 10,20) | 
| -------------- | ----------------------- | ----------------- | -------- |
| Wilcoxon | ★★★★ | ☆ | ☆ |
| T-test | ★★★ | ★★★ | ★★ |
| DESeq2(1) | ☆ | ★★★★★ | ★★★★★ |
| DESeq2(10) | ★ | ★★★★ | ★★★★★ |
| edgeR/TMMwzp+edgeR | ★★★★★ | ★★ | ★★★ |
| TMM+edgeR | ★★★★ | ★★ | ★★★★ |
| RLE+edgeR(0.01) | ★★★★ | ★★★ | ★★★★ |
| UQ+edgeR(0.01) | ☆ | ★★★ | ★★★★ |
| **metagenome** | ★★★★ | ★★★★ | ★★★★★ |
    
(Note: The evaluation result is only meaningful for our generated dataset)

For each method, if we want to compare the stability under different scenarios, we could reorganize the plot as (we only set cutoff = 0.05):  
![nATqZF.png](https://s2.ax1x.com/2019/09/03/nATqZF.png)  
We could find that most methods have a lower power when dealing with Scenario C, except for edgeR/TMMwzp+edgeR. MGS remains a stable performance dealing with different scenarios.


## CONCLUSION

We confirm that metagenome (MGS fitfeaturemodel) is a good method to analyze our simulation data, which can hold low type-I error when no signal is added, and reach high power when obvious signal (10x) is added. In fact, in the real data, 10x is never an exaggerate fold change, we can even see more than 1000x fold change in microbiome data. [Please check this](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxzaWRkaGFydGhhbWFuZGFsMTk4NXxneDo1OGEyMDlkZDBiOWQ2ZmI1). In addition, metagenome method behaves very stable among different scenarios, which means it is not easily influenced by mixed library sizes. Therefore, we could expect metagenome to be appropriate and mighty when dealing with the microbiome sequencing data.

DESeq2 is a tool designed for RNA-seq data, we would perform RLE method as a normalization process. It has high power when there are large differences between two sample groups, but it also have abnormal inflation type-I error rate when no signal is added.  Using larger value as the pseudocount constant will alleviate this sharp problem, but it is after all a tradeoff strategy, because the power would simultaneously drop as well.

edgeR is another common tool for RNA-seq data, 

Two sample T-test is one of the most famous and classic differential test method in history. 

Wilcoxon rank test is the worst test among all. As a nonparametric method, it is especially inappropriate for microbiome data because of the existence of huge amount of zeros. All the zeros would be regarded as "ties" in the Wilcoxon rank test, and since these zeros are widely interspersed among all samples in 2 groups, Wilcoxon rank test could hardly distinguish the difference of two groups. When we try to add signals to one of the groups, Wilcoxon rank test still behaves terribly bad even the signal is very large, because zero times any number is still zero.  
Use a zero imputation method may possibly solve this problem, but it changes the raw data and doesn't have a one-to-one correspondence relationship (bijection). Therefore, we didn't take Wilcoxon test into account when plotting.
